{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords, ):\n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Willing to move back to India?</td>\n",
       "      <td>Samsung Electronics</td>\n",
       "      <td>San Jose, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote Epidemiologist</td>\n",
       "      <td>Insight Global</td>\n",
       "      <td>Denver Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Mondo</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Programming Analyst</td>\n",
       "      <td>Sony Pictures Entertainment</td>\n",
       "      <td>Miami, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Insights Analyst</td>\n",
       "      <td>Daymon</td>\n",
       "      <td>Hutchinson, KS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data and Analytics Engineer</td>\n",
       "      <td>rprt</td>\n",
       "      <td>Buffalo, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst - Risk and Payments</td>\n",
       "      <td>Wish</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst/ Scientist</td>\n",
       "      <td>Claremont Consulting</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Motion Recruitment</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Business Intelligence Analyst</td>\n",
       "      <td>TeamSoft</td>\n",
       "      <td>Milwaukee, WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Sentry Data Systems</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>LTI - Larsen &amp; Toubro Infotech</td>\n",
       "      <td>Irving, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Horizontal Talent</td>\n",
       "      <td>Syracuse, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Brooksource</td>\n",
       "      <td>Nashville, TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Consulting Analyst</td>\n",
       "      <td>The Lab Consulting</td>\n",
       "      <td>Houston, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Analyst</td>\n",
       "      <td>Concentra Analytics</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Millar Cameron</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Scientist - Strategy &amp; BizOps</td>\n",
       "      <td>Instacart</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Engineering - a Hill International Company</td>\n",
       "      <td>Allendale, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Analyst, Market Insight and Analytics</td>\n",
       "      <td>SUN PHARMA</td>\n",
       "      <td>Princeton, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Research Analyst</td>\n",
       "      <td>Ragnar Research Partners</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data and Insights Analyst</td>\n",
       "      <td>Blueair</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Digital Analyst (20 hours / week)</td>\n",
       "      <td>Amy's Kitchen</td>\n",
       "      <td>Petaluma, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Analyst, Gaming Studio</td>\n",
       "      <td>Andiamo Partners</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title  \\\n",
       "0           Willing to move back to India?   \n",
       "1                    Remote Epidemiologist   \n",
       "2                             Data Analyst   \n",
       "3                      Programming Analyst   \n",
       "4                         Insights Analyst   \n",
       "5                           Data Scientist   \n",
       "6              Data and Analytics Engineer   \n",
       "7         Data Analyst - Risk and Payments   \n",
       "8                  Data Analyst/ Scientist   \n",
       "9                             Data Analyst   \n",
       "10           Business Intelligence Analyst   \n",
       "11                            Data Analyst   \n",
       "12                            Data Analyst   \n",
       "13                            Data Analyst   \n",
       "14                   Junior Data Scientist   \n",
       "15                      Consulting Analyst   \n",
       "16                                 Analyst   \n",
       "17                            Data Analyst   \n",
       "18      Data Scientist - Strategy & BizOps   \n",
       "19                     Junior Data Analyst   \n",
       "20  Analyst, Market Insight and Analytics    \n",
       "21                        Research Analyst   \n",
       "22               Data and Insights Analyst   \n",
       "23       Digital Analyst (20 hours / week)   \n",
       "24             Data Analyst, Gaming Studio   \n",
       "\n",
       "                                       Company                  Location  \n",
       "0                          Samsung Electronics              San Jose, CA  \n",
       "1                               Insight Global  Denver Metropolitan Area  \n",
       "2                                        Mondo         San Francisco, CA  \n",
       "3                  Sony Pictures Entertainment                 Miami, FL  \n",
       "4                                       Daymon            Hutchinson, KS  \n",
       "5                                        Apple             Cupertino, CA  \n",
       "6                                         rprt               Buffalo, NY  \n",
       "7                                         Wish         San Francisco, CA  \n",
       "8                         Claremont Consulting                Boston, MA  \n",
       "9                           Motion Recruitment                Boston, MA  \n",
       "10                                    TeamSoft             Milwaukee, WI  \n",
       "11                         Sentry Data Systems             United States  \n",
       "12              LTI - Larsen & Toubro Infotech                Irving, TX  \n",
       "13                           Horizontal Talent              Syracuse, NY  \n",
       "14                                 Brooksource             Nashville, TN  \n",
       "15                          The Lab Consulting               Houston, TX  \n",
       "16                         Concentra Analytics          Philadelphia, PA  \n",
       "17                              Millar Cameron           Los Angeles, CA  \n",
       "18                                   Instacart         San Francisco, CA  \n",
       "19  Engineering - a Hill International Company             Allendale, NJ  \n",
       "20                                  SUN PHARMA             Princeton, NJ  \n",
       "21                    Ragnar Research Partners         Oklahoma City, OK  \n",
       "22                                     Blueair               Chicago, IL  \n",
       "23                               Amy's Kitchen              Petaluma, CA  \n",
       "24                            Andiamo Partners              New York, NY  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Willing to move back to India?</td>\n",
       "      <td>Samsung Electronics</td>\n",
       "      <td>San Jose, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote Epidemiologist</td>\n",
       "      <td>Insight Global</td>\n",
       "      <td>Denver Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Mondo</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Programming Analyst</td>\n",
       "      <td>Sony Pictures Entertainment</td>\n",
       "      <td>Miami, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Insights Analyst</td>\n",
       "      <td>Daymon</td>\n",
       "      <td>Hutchinson, KS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Analyst, Market Insight and Analytics</td>\n",
       "      <td>SUN PHARMA</td>\n",
       "      <td>Princeton, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Research Analyst</td>\n",
       "      <td>Ragnar Research Partners</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Data and Insights Analyst</td>\n",
       "      <td>Blueair</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Digital Analyst (20 hours / week)</td>\n",
       "      <td>Amy's Kitchen</td>\n",
       "      <td>Petaluma, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Data Analyst, Gaming Studio</td>\n",
       "      <td>Andiamo Partners</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title                      Company  \\\n",
       "0            Willing to move back to India?          Samsung Electronics   \n",
       "1                     Remote Epidemiologist               Insight Global   \n",
       "2                              Data Analyst                        Mondo   \n",
       "3                       Programming Analyst  Sony Pictures Entertainment   \n",
       "4                          Insights Analyst                       Daymon   \n",
       "..                                      ...                          ...   \n",
       "120  Analyst, Market Insight and Analytics                    SUN PHARMA   \n",
       "121                        Research Analyst     Ragnar Research Partners   \n",
       "122               Data and Insights Analyst                      Blueair   \n",
       "123       Digital Analyst (20 hours / week)                Amy's Kitchen   \n",
       "124             Data Analyst, Gaming Studio             Andiamo Partners   \n",
       "\n",
       "                     Location  \n",
       "0                San Jose, CA  \n",
       "1    Denver Metropolitan Area  \n",
       "2           San Francisco, CA  \n",
       "3                   Miami, FL  \n",
       "4              Hutchinson, KS  \n",
       "..                        ...  \n",
       "120             Princeton, NJ  \n",
       "121         Oklahoma City, OK  \n",
       "122               Chicago, IL  \n",
       "123              Petaluma, CA  \n",
       "124              New York, NY  \n",
       "\n",
       "[125 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_linkedin_job_search(keywords, num_pages=1):\n",
    "    result_dataframe = pd.DataFrame()\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Create URL addition in case num_pages are provided\n",
    "    for i in range(num_pages):\n",
    "        num_pages_str = '&start=' + str(num_pages * 25)\n",
    "\n",
    "        # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords, num_pages_str])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "        columns = ['Title', 'Company', 'Location']\n",
    "        data = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "        result_dataframe = pd.concat([result_dataframe, data], axis=0)\n",
    "        result_dataframe.reset_index(drop=True, inplace=True)\n",
    "    return result_dataframe\n",
    "\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis', 5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Projects Data Analyst, CDI, H/F</td>\n",
       "      <td>PepsiCo</td>\n",
       "      <td>Colombes, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Research Assistant (Data Management &amp; Analysis)</td>\n",
       "      <td>World Health Organization</td>\n",
       "      <td>Lyon, Auvergne-Rhone-Alpes, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Research Analyst</td>\n",
       "      <td>Statista</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - CDI</td>\n",
       "      <td>Webhelp</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst (F/H)</td>\n",
       "      <td>Challenge2Media (C2m group)</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analytics Consultant</td>\n",
       "      <td>Digital Value</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data analyst</td>\n",
       "      <td>TALENTPEOPLE</td>\n",
       "      <td>Boulogne-Billancourt, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst (H/F)</td>\n",
       "      <td>CleverConnect (Meteojob, Visiotalent, HRmatch)</td>\n",
       "      <td>Sélestat, Grand Est, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Business Analyst Data</td>\n",
       "      <td>VYV3</td>\n",
       "      <td>Angers, Pays de la Loire, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Business Intelligence Analyst</td>\n",
       "      <td>Mantu</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data analysis, BI &amp; reporting</td>\n",
       "      <td>Access Capital Partners</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst H/F</td>\n",
       "      <td>Nickel</td>\n",
       "      <td>Nantes, Pays de la Loire, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Analyst H/F</td>\n",
       "      <td>Ogilvy Paris</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>IQVIA</td>\n",
       "      <td>Saint-Barthélemy-d’Anjou, Pays de la Loire, Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data analyst (M/F) - Full time</td>\n",
       "      <td>POK SAS</td>\n",
       "      <td>Nogent-sur-Seine, Grand Est, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Insights Analyst</td>\n",
       "      <td>InterQuest Group</td>\n",
       "      <td>Massy, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Consultant Data</td>\n",
       "      <td>SIB [numérique et santé]</td>\n",
       "      <td>Rennes, Brittany, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Analyst (H/F)</td>\n",
       "      <td>360Learning</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Analyst, Market Intelligence (m/f/d)</td>\n",
       "      <td>ADM</td>\n",
       "      <td>Aixe-sur-Vienne, Nouvelle-Aquitaine, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Analyst (H/F)</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ESG Analyst</td>\n",
       "      <td>Candriam</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Data Analyst H/F</td>\n",
       "      <td>Gojob</td>\n",
       "      <td>Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bolden</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>IDEMIA</td>\n",
       "      <td>Courbevoie, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Junior Data Analyst (M/F)</td>\n",
       "      <td>Voodoo.io</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Projects Data Analyst, CDI, H/F</td>\n",
       "      <td>PepsiCo</td>\n",
       "      <td>Colombes, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Research Assistant (Data Management &amp; Analysis)</td>\n",
       "      <td>World Health Organization</td>\n",
       "      <td>Lyon, Auvergne-Rhone-Alpes, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Junior Research Analyst</td>\n",
       "      <td>Statista</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data Scientist - CDI</td>\n",
       "      <td>Webhelp</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Analyst (F/H)</td>\n",
       "      <td>Challenge2Media (C2m group)</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Data Analytics Consultant</td>\n",
       "      <td>Digital Value</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Data analyst</td>\n",
       "      <td>TALENTPEOPLE</td>\n",
       "      <td>Boulogne-Billancourt, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Data Analyst (H/F)</td>\n",
       "      <td>CleverConnect (Meteojob, Visiotalent, HRmatch)</td>\n",
       "      <td>Sélestat, Grand Est, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Business Analyst Data</td>\n",
       "      <td>VYV3</td>\n",
       "      <td>Angers, Pays de la Loire, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Business Intelligence Analyst</td>\n",
       "      <td>Mantu</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Data analysis, BI &amp; reporting</td>\n",
       "      <td>Access Capital Partners</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Data Analyst H/F</td>\n",
       "      <td>Nickel</td>\n",
       "      <td>Nantes, Pays de la Loire, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Data Analyst H/F</td>\n",
       "      <td>Ogilvy Paris</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>IQVIA</td>\n",
       "      <td>Saint-Barthélemy-d’Anjou, Pays de la Loire, Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Data analyst (M/F) - Full time</td>\n",
       "      <td>POK SAS</td>\n",
       "      <td>Nogent-sur-Seine, Grand Est, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Insights Analyst</td>\n",
       "      <td>InterQuest Group</td>\n",
       "      <td>Massy, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Consultant Data</td>\n",
       "      <td>SIB [numérique et santé]</td>\n",
       "      <td>Rennes, Brittany, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Data Analyst (H/F)</td>\n",
       "      <td>360Learning</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Analyst, Market Intelligence (m/f/d)</td>\n",
       "      <td>ADM</td>\n",
       "      <td>Aixe-sur-Vienne, Nouvelle-Aquitaine, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Data Analyst (H/F)</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ESG Analyst</td>\n",
       "      <td>Candriam</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Data Analyst H/F</td>\n",
       "      <td>Gojob</td>\n",
       "      <td>Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bolden</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>IDEMIA</td>\n",
       "      <td>Courbevoie, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Junior Data Analyst (M/F)</td>\n",
       "      <td>Voodoo.io</td>\n",
       "      <td>Paris, Ile-de-France, France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "0                   Projects Data Analyst, CDI, H/F   \n",
       "1   Research Assistant (Data Management & Analysis)   \n",
       "2                           Junior Research Analyst   \n",
       "3                              Data Scientist - CDI   \n",
       "4                                Data Analyst (F/H)   \n",
       "5                         Data Analytics Consultant   \n",
       "6                                     Data analyst    \n",
       "7                                Data Analyst (H/F)   \n",
       "8                             Business Analyst Data   \n",
       "9                     Business Intelligence Analyst   \n",
       "10                    Data analysis, BI & reporting   \n",
       "11                                 Data Analyst H/F   \n",
       "12                                 Data Analyst H/F   \n",
       "13                                     Data Analyst   \n",
       "14                   Data analyst (M/F) - Full time   \n",
       "15                                 Insights Analyst   \n",
       "16                                  Consultant Data   \n",
       "17                               Data Analyst (H/F)   \n",
       "18             Analyst, Market Intelligence (m/f/d)   \n",
       "19                               Data Analyst (H/F)   \n",
       "20                                      ESG Analyst   \n",
       "21                                 Data Analyst H/F   \n",
       "22                                     Data Analyst   \n",
       "23                                   Data Scientist   \n",
       "24                        Junior Data Analyst (M/F)   \n",
       "25                  Projects Data Analyst, CDI, H/F   \n",
       "26  Research Assistant (Data Management & Analysis)   \n",
       "27                          Junior Research Analyst   \n",
       "28                             Data Scientist - CDI   \n",
       "29                               Data Analyst (F/H)   \n",
       "30                        Data Analytics Consultant   \n",
       "31                                    Data analyst    \n",
       "32                               Data Analyst (H/F)   \n",
       "33                            Business Analyst Data   \n",
       "34                    Business Intelligence Analyst   \n",
       "35                    Data analysis, BI & reporting   \n",
       "36                                 Data Analyst H/F   \n",
       "37                                 Data Analyst H/F   \n",
       "38                                     Data Analyst   \n",
       "39                   Data analyst (M/F) - Full time   \n",
       "40                                 Insights Analyst   \n",
       "41                                  Consultant Data   \n",
       "42                               Data Analyst (H/F)   \n",
       "43             Analyst, Market Intelligence (m/f/d)   \n",
       "44                               Data Analyst (H/F)   \n",
       "45                                      ESG Analyst   \n",
       "46                                 Data Analyst H/F   \n",
       "47                                     Data Analyst   \n",
       "48                                   Data Scientist   \n",
       "49                        Junior Data Analyst (M/F)   \n",
       "\n",
       "                                           Company  \\\n",
       "0                                          PepsiCo   \n",
       "1                        World Health Organization   \n",
       "2                                         Statista   \n",
       "3                                          Webhelp   \n",
       "4                      Challenge2Media (C2m group)   \n",
       "5                                    Digital Value   \n",
       "6                                     TALENTPEOPLE   \n",
       "7   CleverConnect (Meteojob, Visiotalent, HRmatch)   \n",
       "8                                             VYV3   \n",
       "9                                            Mantu   \n",
       "10                         Access Capital Partners   \n",
       "11                                          Nickel   \n",
       "12                                    Ogilvy Paris   \n",
       "13                                           IQVIA   \n",
       "14                                         POK SAS   \n",
       "15                                InterQuest Group   \n",
       "16                        SIB [numérique et santé]   \n",
       "17                                     360Learning   \n",
       "18                                             ADM   \n",
       "19                                           CELAD   \n",
       "20                                        Candriam   \n",
       "21                                           Gojob   \n",
       "22                                          Bolden   \n",
       "23                                          IDEMIA   \n",
       "24                                       Voodoo.io   \n",
       "25                                         PepsiCo   \n",
       "26                       World Health Organization   \n",
       "27                                        Statista   \n",
       "28                                         Webhelp   \n",
       "29                     Challenge2Media (C2m group)   \n",
       "30                                   Digital Value   \n",
       "31                                    TALENTPEOPLE   \n",
       "32  CleverConnect (Meteojob, Visiotalent, HRmatch)   \n",
       "33                                            VYV3   \n",
       "34                                           Mantu   \n",
       "35                         Access Capital Partners   \n",
       "36                                          Nickel   \n",
       "37                                    Ogilvy Paris   \n",
       "38                                           IQVIA   \n",
       "39                                         POK SAS   \n",
       "40                                InterQuest Group   \n",
       "41                        SIB [numérique et santé]   \n",
       "42                                     360Learning   \n",
       "43                                             ADM   \n",
       "44                                           CELAD   \n",
       "45                                        Candriam   \n",
       "46                                           Gojob   \n",
       "47                                          Bolden   \n",
       "48                                          IDEMIA   \n",
       "49                                       Voodoo.io   \n",
       "\n",
       "                                             Location  \n",
       "0                     Colombes, Ile-de-France, France  \n",
       "1                  Lyon, Auvergne-Rhone-Alpes, France  \n",
       "2                        Paris, Ile-de-France, France  \n",
       "3                        Paris, Ile-de-France, France  \n",
       "4                        Paris, Ile-de-France, France  \n",
       "5                        Paris, Ile-de-France, France  \n",
       "6         Boulogne-Billancourt, Ile-de-France, France  \n",
       "7                         Sélestat, Grand Est, France  \n",
       "8                    Angers, Pays de la Loire, France  \n",
       "9                        Paris, Ile-de-France, France  \n",
       "10                                             France  \n",
       "11                   Nantes, Pays de la Loire, France  \n",
       "12                                             France  \n",
       "13  Saint-Barthélemy-d’Anjou, Pays de la Loire, Fr...  \n",
       "14                Nogent-sur-Seine, Grand Est, France  \n",
       "15                       Massy, Ile-de-France, France  \n",
       "16                           Rennes, Brittany, France  \n",
       "17                       Paris, Ile-de-France, France  \n",
       "18        Aixe-sur-Vienne, Nouvelle-Aquitaine, France  \n",
       "19  Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...  \n",
       "20                                             France  \n",
       "21  Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...  \n",
       "22                       Paris, Ile-de-France, France  \n",
       "23                  Courbevoie, Ile-de-France, France  \n",
       "24                       Paris, Ile-de-France, France  \n",
       "25                    Colombes, Ile-de-France, France  \n",
       "26                 Lyon, Auvergne-Rhone-Alpes, France  \n",
       "27                       Paris, Ile-de-France, France  \n",
       "28                       Paris, Ile-de-France, France  \n",
       "29                       Paris, Ile-de-France, France  \n",
       "30                       Paris, Ile-de-France, France  \n",
       "31        Boulogne-Billancourt, Ile-de-France, France  \n",
       "32                        Sélestat, Grand Est, France  \n",
       "33                   Angers, Pays de la Loire, France  \n",
       "34                       Paris, Ile-de-France, France  \n",
       "35                                             France  \n",
       "36                   Nantes, Pays de la Loire, France  \n",
       "37                                             France  \n",
       "38  Saint-Barthélemy-d’Anjou, Pays de la Loire, Fr...  \n",
       "39                Nogent-sur-Seine, Grand Est, France  \n",
       "40                       Massy, Ile-de-France, France  \n",
       "41                           Rennes, Brittany, France  \n",
       "42                       Paris, Ile-de-France, France  \n",
       "43        Aixe-sur-Vienne, Nouvelle-Aquitaine, France  \n",
       "44  Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...  \n",
       "45                                             France  \n",
       "46  Aix-en-Provence, Provence-Alpes-Cote d'Azur, F...  \n",
       "47                       Paris, Ile-de-France, France  \n",
       "48                  Courbevoie, Ile-de-France, France  \n",
       "49                       Paris, Ile-de-France, France  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_linkedin_job_search(keywords, num_pages=1, country='Germany'):\n",
    "    result_dataframe = pd.DataFrame()\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Create URL addition in case num_pages are provided\n",
    "    for i in range(num_pages):\n",
    "        num_pages_str = '&start=' + str(num_pages * 25)\n",
    "        \n",
    "        # Create country string\n",
    "        country_str = '&location=' + str(country)\n",
    "\n",
    "        # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords, num_pages_str, country_str])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "        columns = ['Title', 'Company', 'Location']\n",
    "        data = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "        result_dataframe = pd.concat([result_dataframe, data], axis=0)\n",
    "        result_dataframe.reset_index(drop=True, inplace=True)\n",
    "    return result_dataframe\n",
    "\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis', num_pages=2, country='France')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Willing to move back to India?</td>\n",
       "      <td>Samsung Electronics</td>\n",
       "      <td>San Jose, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remote Epidemiologist</td>\n",
       "      <td>Insight Global</td>\n",
       "      <td>Denver Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Mondo</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Programming Analyst</td>\n",
       "      <td>Sony Pictures Entertainment</td>\n",
       "      <td>Miami, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Insights Analyst</td>\n",
       "      <td>Daymon</td>\n",
       "      <td>Hutchinson, KS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data and Analytics Engineer</td>\n",
       "      <td>rprt</td>\n",
       "      <td>Buffalo, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst - Risk and Payments</td>\n",
       "      <td>Wish</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst/ Scientist</td>\n",
       "      <td>Claremont Consulting</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Motion Recruitment</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Business Intelligence Analyst</td>\n",
       "      <td>TeamSoft</td>\n",
       "      <td>Milwaukee, WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Sentry Data Systems</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>LTI - Larsen &amp; Toubro Infotech</td>\n",
       "      <td>Irving, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Horizontal Talent</td>\n",
       "      <td>Syracuse, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Brooksource</td>\n",
       "      <td>Nashville, TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Consulting Analyst</td>\n",
       "      <td>The Lab Consulting</td>\n",
       "      <td>Houston, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Analyst</td>\n",
       "      <td>Concentra Analytics</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Millar Cameron</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Scientist - Strategy &amp; BizOps</td>\n",
       "      <td>Instacart</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Engineering - a Hill International Company</td>\n",
       "      <td>Allendale, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Analyst, Market Insight and Analytics</td>\n",
       "      <td>SUN PHARMA</td>\n",
       "      <td>Princeton, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Research Analyst</td>\n",
       "      <td>Ragnar Research Partners</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Data and Insights Analyst</td>\n",
       "      <td>Blueair</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Digital Analyst (20 hours / week)</td>\n",
       "      <td>Amy's Kitchen</td>\n",
       "      <td>Petaluma, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Analyst, Gaming Studio</td>\n",
       "      <td>Andiamo Partners</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Willing to move back to India?</td>\n",
       "      <td>Samsung Electronics</td>\n",
       "      <td>San Jose, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Remote Epidemiologist</td>\n",
       "      <td>Insight Global</td>\n",
       "      <td>Denver Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Mondo</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Programming Analyst</td>\n",
       "      <td>Sony Pictures Entertainment</td>\n",
       "      <td>Miami, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Insights Analyst</td>\n",
       "      <td>Daymon</td>\n",
       "      <td>Hutchinson, KS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Data and Analytics Engineer</td>\n",
       "      <td>rprt</td>\n",
       "      <td>Buffalo, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Data Analyst - Risk and Payments</td>\n",
       "      <td>Wish</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Data Analyst/ Scientist</td>\n",
       "      <td>Claremont Consulting</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Motion Recruitment</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Business Intelligence Analyst</td>\n",
       "      <td>TeamSoft</td>\n",
       "      <td>Milwaukee, WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Sentry Data Systems</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>LTI - Larsen &amp; Toubro Infotech</td>\n",
       "      <td>Irving, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Horizontal Talent</td>\n",
       "      <td>Syracuse, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Brooksource</td>\n",
       "      <td>Nashville, TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Consulting Analyst</td>\n",
       "      <td>The Lab Consulting</td>\n",
       "      <td>Houston, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Analyst</td>\n",
       "      <td>Concentra Analytics</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Millar Cameron</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Data Scientist - Strategy &amp; BizOps</td>\n",
       "      <td>Instacart</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Engineering - a Hill International Company</td>\n",
       "      <td>Allendale, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Analyst, Market Insight and Analytics</td>\n",
       "      <td>SUN PHARMA</td>\n",
       "      <td>Princeton, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Research Analyst</td>\n",
       "      <td>Ragnar Research Partners</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Data and Insights Analyst</td>\n",
       "      <td>Blueair</td>\n",
       "      <td>Chicago, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Digital Analyst (20 hours / week)</td>\n",
       "      <td>Amy's Kitchen</td>\n",
       "      <td>Petaluma, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Data Analyst, Gaming Studio</td>\n",
       "      <td>Andiamo Partners</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title  \\\n",
       "0           Willing to move back to India?   \n",
       "1                    Remote Epidemiologist   \n",
       "2                             Data Analyst   \n",
       "3                      Programming Analyst   \n",
       "4                           Data Scientist   \n",
       "5                         Insights Analyst   \n",
       "6              Data and Analytics Engineer   \n",
       "7         Data Analyst - Risk and Payments   \n",
       "8                  Data Analyst/ Scientist   \n",
       "9                             Data Analyst   \n",
       "10           Business Intelligence Analyst   \n",
       "11                            Data Analyst   \n",
       "12                            Data Analyst   \n",
       "13                            Data Analyst   \n",
       "14                   Junior Data Scientist   \n",
       "15                      Consulting Analyst   \n",
       "16                                 Analyst   \n",
       "17                            Data Analyst   \n",
       "18      Data Scientist - Strategy & BizOps   \n",
       "19                     Junior Data Analyst   \n",
       "20  Analyst, Market Insight and Analytics    \n",
       "21                        Research Analyst   \n",
       "22               Data and Insights Analyst   \n",
       "23       Digital Analyst (20 hours / week)   \n",
       "24             Data Analyst, Gaming Studio   \n",
       "25          Willing to move back to India?   \n",
       "26                   Remote Epidemiologist   \n",
       "27                            Data Analyst   \n",
       "28                     Programming Analyst   \n",
       "29                          Data Scientist   \n",
       "30                        Insights Analyst   \n",
       "31             Data and Analytics Engineer   \n",
       "32        Data Analyst - Risk and Payments   \n",
       "33                 Data Analyst/ Scientist   \n",
       "34                            Data Analyst   \n",
       "35           Business Intelligence Analyst   \n",
       "36                            Data Analyst   \n",
       "37                            Data Analyst   \n",
       "38                            Data Analyst   \n",
       "39                   Junior Data Scientist   \n",
       "40                      Consulting Analyst   \n",
       "41                                 Analyst   \n",
       "42                            Data Analyst   \n",
       "43      Data Scientist - Strategy & BizOps   \n",
       "44                     Junior Data Analyst   \n",
       "45  Analyst, Market Insight and Analytics    \n",
       "46                        Research Analyst   \n",
       "47               Data and Insights Analyst   \n",
       "48       Digital Analyst (20 hours / week)   \n",
       "49             Data Analyst, Gaming Studio   \n",
       "\n",
       "                                       Company                  Location  \n",
       "0                          Samsung Electronics              San Jose, CA  \n",
       "1                               Insight Global  Denver Metropolitan Area  \n",
       "2                                        Mondo         San Francisco, CA  \n",
       "3                  Sony Pictures Entertainment                 Miami, FL  \n",
       "4                                        Apple             Cupertino, CA  \n",
       "5                                       Daymon            Hutchinson, KS  \n",
       "6                                         rprt               Buffalo, NY  \n",
       "7                                         Wish         San Francisco, CA  \n",
       "8                         Claremont Consulting                Boston, MA  \n",
       "9                           Motion Recruitment                Boston, MA  \n",
       "10                                    TeamSoft             Milwaukee, WI  \n",
       "11                         Sentry Data Systems             United States  \n",
       "12              LTI - Larsen & Toubro Infotech                Irving, TX  \n",
       "13                           Horizontal Talent              Syracuse, NY  \n",
       "14                                 Brooksource             Nashville, TN  \n",
       "15                          The Lab Consulting               Houston, TX  \n",
       "16                         Concentra Analytics          Philadelphia, PA  \n",
       "17                              Millar Cameron           Los Angeles, CA  \n",
       "18                                   Instacart         San Francisco, CA  \n",
       "19  Engineering - a Hill International Company             Allendale, NJ  \n",
       "20                                  SUN PHARMA             Princeton, NJ  \n",
       "21                    Ragnar Research Partners         Oklahoma City, OK  \n",
       "22                                     Blueair               Chicago, IL  \n",
       "23                               Amy's Kitchen              Petaluma, CA  \n",
       "24                            Andiamo Partners              New York, NY  \n",
       "25                         Samsung Electronics              San Jose, CA  \n",
       "26                              Insight Global  Denver Metropolitan Area  \n",
       "27                                       Mondo         San Francisco, CA  \n",
       "28                 Sony Pictures Entertainment                 Miami, FL  \n",
       "29                                       Apple             Cupertino, CA  \n",
       "30                                      Daymon            Hutchinson, KS  \n",
       "31                                        rprt               Buffalo, NY  \n",
       "32                                        Wish         San Francisco, CA  \n",
       "33                        Claremont Consulting                Boston, MA  \n",
       "34                          Motion Recruitment                Boston, MA  \n",
       "35                                    TeamSoft             Milwaukee, WI  \n",
       "36                         Sentry Data Systems             United States  \n",
       "37              LTI - Larsen & Toubro Infotech                Irving, TX  \n",
       "38                           Horizontal Talent              Syracuse, NY  \n",
       "39                                 Brooksource             Nashville, TN  \n",
       "40                          The Lab Consulting               Houston, TX  \n",
       "41                         Concentra Analytics          Philadelphia, PA  \n",
       "42                              Millar Cameron           Los Angeles, CA  \n",
       "43                                   Instacart         San Francisco, CA  \n",
       "44  Engineering - a Hill International Company             Allendale, NJ  \n",
       "45                                  SUN PHARMA             Princeton, NJ  \n",
       "46                    Ragnar Research Partners         Oklahoma City, OK  \n",
       "47                                     Blueair               Chicago, IL  \n",
       "48                               Amy's Kitchen              Petaluma, CA  \n",
       "49                            Andiamo Partners              New York, NY  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_linkedin_job_search(keywords, num_pages=1, country='Germany', num_days=1):\n",
    "    result_dataframe = pd.DataFrame()\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Create country string\n",
    "    country_str = '&location=' + str(country)\n",
    "    \n",
    "    # Create time string\n",
    "    num_secs = num_days * 24 * 60 * 60\n",
    "    time_str = 'f_TPR=r' + str(num_secs)\n",
    "    \n",
    "    # Create URL addition in case num_pages are provided\n",
    "    for i in range(num_pages):\n",
    "        num_pages_str = '&start=' + str(num_pages * 25)\n",
    "        \n",
    "\n",
    "\n",
    "        # Assemble the full url with parameters\n",
    "        scrape_url = ''.join([BASE_URL, 'keywords=', keywords, num_pages_str, country_str, time_str])\n",
    "\n",
    "        # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "        columns = ['Title', 'Company', 'Location']\n",
    "        data = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "        # Then in each job card, extract the job title, company, and location data.\n",
    "        titles = []\n",
    "        companies = []\n",
    "        locations = []\n",
    "        for card in soup.select(\"div.result-card__contents\"):\n",
    "            title = card.findChild(\"h3\", recursive=False)\n",
    "            company = card.findChild(\"h4\", recursive=False)\n",
    "            location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "            titles.append(title.string)\n",
    "            companies.append(company.string)\n",
    "            locations.append(location.string)\n",
    "\n",
    "        # Inject job titles, companies, and locations into the empty dataframe\n",
    "        zipped = zip(titles, companies, locations)\n",
    "        for z in list(zipped):\n",
    "            data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "        result_dataframe = pd.concat([result_dataframe, data], axis=0)\n",
    "        result_dataframe.reset_index(drop=True, inplace=True)\n",
    "    return result_dataframe\n",
    "\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis', num_pages=2, country='France', num_days=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
